import os
import pickle
import numpy as np
import pandas as pd
import openai
from sklearn.metrics.pairwise import cosine_similarity
from kiwipiepy import Kiwi
import re
from kiwipiepy.utils import Stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = ""
client = openai.OpenAI() 

db = pd.read_excel("processed_final_with_summaries.xlsx") # ê¸°ì¡´ ê³µê³  ìš”ì•½ DB


# DBì—ì„œ ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰ í•¨ìˆ˜
def search_db(user_full_job, db):
    """
    DBì—ì„œ user_full_jobì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    filtered_data = db[
        db['job'].str.contains(user_full_job, na=False)  # user_full_jobì„ 'job' ì—´ì—ì„œ ê²€ìƒ‰
    ]
    if not filtered_data.empty:
        return filtered_data.iloc[0]  # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë°ì´í„° ë°˜í™˜
    else:
        return None


#############################################################
#### ì±„ìš© ê³µê³  ìƒì„± 

def generate_job_posting(job, company):
    """
    1. ì‚¬ìš©ì ì…ë ¥(ì§ë¬´, íšŒì‚¬)ì— ê¸°ë°˜í•œ 'ì±„ìš© ê³µê³  ìš”ì•½' í”„ë¡¬í”„íŠ¸ ìƒì„±
    2. client.chat.completions.create()ë¡œ GPT API í˜¸ì¶œ â†’ ì±„ìš© ê³µê³ (ì´ˆì•ˆ) ìƒì„±
    """

    # ê¸°ì¡´ DBì—ì„œ ê²€ìƒ‰
    db_data = search_db(user_full_job,db)
    
    if db_data is not None:
        # DBì—ì„œ ê²€ìƒ‰ëœ ë°ì´í„° í™œìš©
        org_summary = db_data['org_sum']
        work_summary = db_data['work_sum']
        skills_summary = db_data['skills_sum']
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ê¸°ì¡´ ë°ì´í„° í™œìš©)
        prompt = (
            f"""
            ì•„ë˜ëŠ” {company}ì˜ {job} ì§ë¬´ì™€ ê´€ë ¨ëœ ê¸°ì¡´ ë°ì´í„°ì…ë‹ˆë‹¤:
            
            1. ì¡°ì§ ì„¤ëª…: {org_summary}
            2. ì§ë¬´ ì„¤ëª…: {work_summary}
            3. í•„ìš” ì—­ëŸ‰: {skills_summary}
            
            ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë” ì™„ì„±ë„ ë†’ì€ ì±„ìš© ê³µê³ ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
            ë¶€ì¡±í•œ ë‚´ìš©ì€ ë³´ì™„í•˜ê³ , ì „ë¬¸ì ì´ê³  ì§ë¬´ì— ì í•©í•œ ë¬¸êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.
            
            **ì¶œë ¥ í˜•ì‹**:
            1. ì¡°ì§ ì„¤ëª…: [ìˆ˜ì • ë˜ëŠ” ë³´ì™„ëœ ì¡°ì§ ì„¤ëª…]
            2. ì§ë¬´ ì„¤ëª…: [ìˆ˜ì • ë˜ëŠ” ë³´ì™„ëœ ì§ë¬´ ì„¤ëª…]
            3. í•„ìš” ì—­ëŸ‰: [ìˆ˜ì • ë˜ëŠ” ë³´ì™„ëœ í•„ìš” ì—­ëŸ‰]
            """
        )
    else:
        # DBì—ì„œ ê²€ìƒ‰ëœ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        prompt = (
        f"""
        ì•„ë˜ì˜ ì§ë¬´ì™€ íšŒì‚¬ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±„ìš© ê³µê³  ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”. ì•„ë˜ëŠ” ì°¸ê³  ì˜ˆì œì…ë‹ˆë‹¤:

        [ì˜ˆì œ1]
        ì§ë¬´: ì‡¼ì½œë¼í‹°ì—
				íšŒì‚¬: ì‹ ë¼í˜¸í…”
				ì¡°ì§ ì„¤ëª…: ì‹ ë¼í˜¸í…”ì˜ ë””ì €íŠ¸ ì œì‘ íŒ€ì€ ê³ ê¸‰ ë””ì €íŠ¸ ë¬¸í™”ë¥¼ ì„ ë„í•˜ë©°, ê³ ê°ì—ê²Œ ë…ì°½ì ì´ê³  ì •êµí•œ ì´ˆì½œë¦¿ ê²½í—˜ì„ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. 
								 ì´ íŒ€ì€ ì´ˆì½œë¦¿ ì¡°ê°ê³¼ ê³µì˜ˆë¥¼ í†µí•´ í˜¸í…”ì˜ í’ˆê²©ì„ ë°˜ì˜í•˜ëŠ” ì‘í’ˆì„ ì œì‘í•˜ë©°, ë§ì¶¤í˜• ë””ì €íŠ¸ì™€ íŠ¹ë³„í•œ í–‰ì‚¬ë¥¼ ìœ„í•œ ì°½ì˜ì ì¸ ì´ˆì½œë¦¿ ì•„íŠ¸ë¥¼ ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.

				ì§ë¬´ ì„¤ëª…: ì‡¼ì½œë¼í‹°ì—ëŠ” ì‹ ë¼í˜¸í…”ì˜ ê³ ê¸‰ ì´ˆì½œë¦¿ ë””ì €íŠ¸ì™€ ì˜ˆìˆ  ì‘í’ˆì„ ê¸°íší•˜ê³  ì œì‘í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. ì£¼ìš” ì—…ë¬´ì—ëŠ” ê³ ê° ë§ì¶¤í˜• ì´ˆì½œë¦¿ ë””ìì¸, ëŒ€ê·œëª¨ í–‰ì‚¬ ë° ì›¨ë”© ì´ˆì½œë¦¿ ì„¸íŠ¸ ì œì‘, ê³„ì ˆë³„ ì‹ ì œí’ˆ ê°œë°œ, 
								 ê·¸ë¦¬ê³  ì´ˆì½œë¦¿ í’ˆì§ˆ ê´€ë¦¬ì™€ ë””ìŠ¤í”Œë ˆì´ ì¤€ë¹„ê°€ í¬í•¨ë©ë‹ˆë‹¤. ë˜í•œ, í˜¸í…” ë¸Œëœë“œë¥¼ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” ì´ˆì½œë¦¿ ì‘í’ˆ ê°œë°œê³¼ êµ­ì œ ì´ˆì½œë¦¿ ê³µëª¨ì „ ì¶œí’ˆì„ ìœ„í•œ ì°½ì˜ì  ë””ìì¸ë„ í¬í•¨ë©ë‹ˆë‹¤.

				í•„ìš” ì—­ëŸ‰: ì´ˆì½œë¦¿ ê³µì˜ˆ ë° ë””ì €íŠ¸ ì œì‘ ë¶„ì•¼ì—ì„œ í’ë¶€í•œ ê²½í—˜ê³¼ ì „ë¬¸ì„±ì„ ê°–ì¶˜ ì§€ì›ìë¥¼ ì°¾ìŠµë‹ˆë‹¤. ì´ˆì½œë¦¿ ì‘ì—…ì— í•„ìš”í•œ ì¬ë£Œ íŠ¹ì„±ì— ëŒ€í•œ ê¹Šì€ ì´í•´ì™€ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ í’ˆì§ˆ ê´€ë¦¬ ëŠ¥ë ¥ì´ ìš”êµ¬ë˜ë©°, 
								 ì°½ì˜ì ì´ê³  ë…ì°½ì ì¸ ì´ˆì½œë¦¿ ë””ìì¸ì„ ê°œë°œí•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì´ í•„ìš”í•©ë‹ˆë‹¤. ë””ì €íŠ¸ ë° ì´ˆì½œë¦¿ ê´€ë ¨ ê³µëª¨ì „ì—ì„œ ìˆ˜ìƒí•œ ê²½ë ¥ì´ë‚˜ ì´ë¥¼ ì¦ëª…í•  ìˆ˜ ìˆëŠ” í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë³´ìœ í•œ ì§€ì›ìë¥¼ ìš°ëŒ€í•˜ë©°, 
								 ëŒ€ê·œëª¨ í–‰ì‚¬ ë° ê³ ê° ë§ì¶¤í˜• ì´ˆì½œë¦¿ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ê°€ì§„ ì§€ì›ìëŠ” ë”ìš± í™˜ì˜í•©ë‹ˆë‹¤.

		[ì˜ˆì œ2]
        ì§ë¬´: AIMLì‡¼í•‘ê²€ìƒ‰ê°œì¸í™”ê¸°ìˆ ì—°êµ¬ê°œë°œ
        íšŒì‚¬: ë„¤ì´ë²„ì‡¼í•‘
        ì¡°ì§ ì„¤ëª…: í•´ë‹¹ ì¡°ì§ì€ ê°œì¸í™”ëœ ì‡¼í•‘ ê²€ìƒ‰ ê²½í—˜ì„ êµ¬í˜„í•˜ëŠ” íŒ€ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì·¨í–¥ê³¼ ì´ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë§ì¶¤í˜• ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. 
                ì´ íŒ€ì€ ë¸Œëœë“œ ì„ í˜¸ë„, ê°€ê²©, ìŠ¤íƒ€ì¼, êµ¬ë§¤ íŒ¨í„´ ë“± ë‹¤ì–‘í•œ ì‹ í˜¸ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë” íš¨ìœ¨ì ì´ê³  íƒìƒ‰ì ì¸ ì‡¼í•‘ ê²½í—˜ì„ ì œê³µí•˜ëŠ” ë° ì¤‘ì ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤.
        ì§ë¬´ ì„¤ëª…: ì§ë¬´ëŠ” ì‚¬ìš©ì ì·¨í–¥ ë° ì˜ë„ë¥¼ ë°˜ì˜í•œ ê°œì¸í™” ê²€ìƒ‰ ì¶”ì²œ ëª¨ë¸ì„ ì„¤ê³„, ê°œë°œ ë° ê³ ë„í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
                ì£¼ìš” ì—…ë¬´ì—ëŠ” ëŒ€ê·œëª¨ ë¡œê·¸ ë¶„ì„ì„ í†µí•œ íŠ¹ì„± ì¶”ì¶œ, ëª¨ë¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§, AB í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì§€í‘œ ëª¨ë‹ˆí„°ë§ ë° ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ê°€ í¬í•¨ë©ë‹ˆë‹¤. ë˜í•œ, ì¶”ì²œ ì‹œìŠ¤í…œê³¼ ê´€ë ¨ëœ ì—°êµ¬ ê°œë°œ ë° AIML ê¸°ë°˜ ì¶”ì²œ ëª¨ë¸ì˜ ì„œë¹„ìŠ¤ ì ìš©ë„ í¬í•¨ë©ë‹ˆë‹¤.
        í•„ìš” ì—­ëŸ‰: ì§ë¬´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” AIML ê¸°ë°˜ ì¶”ì²œ ëª¨ë¸ ê°œë°œ ë° ì„œë¹„ìŠ¤ ê²½í—˜ì´ 3ë…„ ì´ìƒ í•„ìš”í•˜ë©°, 
                ì‚¬ìš©ì ë¶„ì„, ì½˜í…ì¸  ì´í•´, ëª¨ë¸ë§ ë¡œì§ ì„¤ê³„ ë“± ë¬¸ì œ ì •ì˜ ë° í•´ê²° ëŠ¥ë ¥ì´ ìš”êµ¬ë©ë‹ˆë‹¤. 
                ë˜í•œ, LLM ìµœì‹  ê¸°ìˆ  ë° NLP, RecSys ê´€ë ¨ ê¸°ìˆ  í™œìš© ê²½í—˜ì´ í•„ìš”í•˜ë©°, 
                ê²€ìƒ‰ ì¶”ì²œ ê´€ë ¨ í•™íšŒì— ë…¼ë¬¸ì„ ê²Œì¬í•˜ê±°ë‚˜ ì˜¤í”ˆì†ŒìŠ¤ì— ê¸°ì—¬í•œ ê²½í—˜ì´ ìˆëŠ” ê²ƒì´ ë°”ëŒì§í•©ë‹ˆë‹¤.

        **ì…ë ¥**:
        ì§ë¬´ ì´ë¦„: {job}
        íšŒì‚¬ ì´ë¦„: {company}

        **ì¶œë ¥ í˜•ì‹**:
        1. ì¡°ì§ ì„¤ëª…: [í•´ë‹¹ íšŒì‚¬ ë° ì§ë¬´ê°€ í¬í•¨ëœ íŒ€ì´ë‚˜ ì¡°ì§ì˜ ì—­í• , ëª©í‘œ ë° ì„±ê²©ì„ ëª…í™•íˆ ì„œìˆ ]
        2. ì§ë¬´ ì„¤ëª…: [ì…ë ¥ëœ ì§ë¬´ì™€ ê´€ë ¨ëœ ì£¼ìš” ì—…ë¬´, ì±…ì„, ê·¸ë¦¬ê³  ê¸°ëŒ€ë˜ëŠ” í™œë™ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ ]
        3. í•„ìš” ì—­ëŸ‰: [ì§ë¬´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ìš”êµ¬ë˜ëŠ” ê¸°ìˆ ì (ì˜ˆ: íŠ¹ì • ì†Œí”„íŠ¸ì›¨ì–´ë‚˜ íˆ´ ì‚¬ìš© ëŠ¥ë ¥) ë° ë¹„ê¸°ìˆ ì  ì—­ëŸ‰(ì˜ˆ: ì†Œí†µ ëŠ¥ë ¥, ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ ë“±)ì„ ìƒì„¸íˆ ì„œìˆ ]
        """
    )
    

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            temperature=0
        )
        generated_posting_text = response.choices[0].message.content.strip()
        return generated_posting_text
    except Exception as e:
        return f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"



###############################################################################
# 2. ìƒì„±ëœ ì±„ìš© ê³µê³ (í…ìŠ¤íŠ¸)ë¥¼ Upstage solar ì„ë² ë”© ëª¨ë¸ë¡œ ì„ë² ë”©í•œë‹¤.
###############################################################################
from sklearn.metrics.pairwise import cosine_similarity
from openai import OpenAI  # Requires openai==1.52.2

# Upstage solar ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ API ë¶ˆëŸ¬ì˜¤ê¸°
client2 = OpenAI(
    api_key="",  # ë³¸ì¸ì˜ OpenAI API í‚¤ ì…ë ¥
    base_url="https://api.upstage.ai/v1/solar"
)


# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± í•¨ìˆ˜
def get_embedding(text):
    if not text.strip():  # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
        raise ValueError("Input text is empty.")
    
    response = client2.embeddings.create(input=text, model="embedding-query")
    return response.data[0].embedding

###############################################################################
# 3. ê¸°ì¡´ DB(embeddings.pkl)ì— ì €ì¥ëœ ê° ê³µê³ ì˜ ì„ë² ë”©ê³¼ (3)ë²ˆì—ì„œ êµ¬í•œ ì„ë² ë”© ê°„
#    ìœ ì‚¬ë„ë¥¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.
###############################################################################
def retrieval(result, top_n=3):

    user_org, user_work, user_skills = "", "", ""
    for line in result.split("\n"):
        if "ì¡°ì§ ì„¤ëª…" in line:
            user_org = line.split(":", 1)[-1].strip()
        elif "ì§ë¬´ ì„¤ëª…" in line:
            user_work = line.split(":", 1)[-1].strip()
        elif "í•„ìš” ì—­ëŸ‰" in line:
            user_skills = line.split(":", 1)[-1].strip()

    # ê¸°ì¡´ ì„ë² ë”© ë¡œë“œ
    with open('embeddings.pkl', 'rb') as f:
        data = pickle.load(f)

    # ìœ ì € ì…ë ¥ í…ìŠ¤íŠ¸ ì„ë² ë”©
    org_emb = np.array(get_embedding(user_org)).reshape(1, -1)
    work_emb = np.array(get_embedding(user_work)).reshape(1, -1)
    skills_emb = np.array(get_embedding(user_skills)).reshape(1, -1)

    # ì—´ë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    cosim_org = cosine_similarity(org_emb, data["org_sum"])[0]       # shape: (num_jobs,)
    cosim_work = cosine_similarity(work_emb, data["work_sum"])[0]    # shape: (num_jobs,)
    cosim_skills = cosine_similarity(skills_emb, data["skills_sum"])[0]  # shape: (num_jobs,)


    org_top_indices = np.argsort(-cosim_org)[:top_n]
    work_top_indices = np.argsort(-cosim_work)[:top_n]
    skills_top_indices = np.argsort(-cosim_skills)[:top_n]

    candidates_indices = list(set(org_top_indices) | set(work_top_indices) | set(skills_top_indices))

    weights = {"org": 0.2, "work": 0.4, "skills": 0.4}
    final_scores = []
    for idx in candidates_indices:
        score = (
            weights['org'] * cosim_org[idx]
            + weights['work'] * cosim_work[idx]
            + weights['skills'] * cosim_skills[idx]
        )
        final_scores.append((idx, score))
    
    final_scores_sorted = sorted(final_scores, key=lambda x: x[1], reverse=True)
    rerank_top = final_scores_sorted[:top_n]
    rerank_idx = [x[0] for x in rerank_top]

    #return top_indices
    return rerank_idx


###############################################################################
# 5. 1ë²ˆì—ì„œ ìƒì„±ëœ ê³µê³ ì™€ 4ë²ˆì—ì„œ ì¶”ì¶œí•œ ê¸°ì¡´ ê³µê³ ë“¤ì„ í•¨ê»˜ í™œìš©í•˜ì—¬,
#    ìµœì¢…ì ìœ¼ë¡œ í•„ìš”í•œ ì—­ëŸ‰(ê¸°ìˆ ì  3ê°œ, ë¹„ê¸°ìˆ ì  3ê°œ)ì„ ë‹¤ì‹œ GPT ëª¨ë¸ì—ê²Œ ìš”ì²­í•œë‹¤.
###############################################################################
def get_required_skills(job, job_posting, similar_sum):
    
    prompt = (
    f"""
    ì•„ë˜ëŠ” ìƒˆë¡œ ìƒì„±ëœ '{job}'ì— ê´€í•œ ì±„ìš© ê³µê³ ì™€ ê¸°ì¡´ ìœ ì‚¬ ê³µê³ ì…ë‹ˆë‹¤. 
    ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ë¬´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì—­ëŸ‰(ê¸°ìˆ ì  3ê°œ, ë¹„ê¸°ìˆ ì  3ê°œ)ì„ êµ¬ì²´ì ìœ¼ë¡œ ë„ì¶œí•˜ì„¸ìš”.

    [ìƒˆë¡œ ìƒì„±ëœ ê³µê³ ]
    '{job_posting}'

    [ê¸°ì¡´ ìœ ì‚¬ ê³µê³ ]
    1. '{similar_sum[0]}'
    2. '{similar_sum[1]}'
    3. '{similar_sum[2]}'

    **ì¶œë ¥ í˜•ì‹**
    1. ê¸°ìˆ ì  ì—­ëŸ‰:
    - [ì—­ëŸ‰1: ì„¤ëª…]
    - [ì—­ëŸ‰2: ì„¤ëª…]
    - [ì—­ëŸ‰3: ì„¤ëª…]

    2. ë¹„ê¸°ìˆ ì  ì—­ëŸ‰:
    - [ì—­ëŸ‰1: ì„¤ëª…]
    - [ì—­ëŸ‰2: ì„¤ëª…]
    - [ì—­ëŸ‰3: ì„¤ëª…]
    """
)
    try:
        final_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            temperature=0
        )
        user_skills = final_response.choices[0].message.content.strip()
        return user_skills
        
    except Exception as e:
        return f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    


def generate_personal_statement(job, selected_categories, activities, skills):
    """
    ì‚¬ìš©ìê°€ ì„ íƒí•œ í•­ëª©ê³¼ ì…ë ¥í•œ í™œë™, ê·¸ë¦¬ê³  RAGì—ì„œ ì¶”ì¶œëœ í•„ìš” ì—­ëŸ‰ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸€ê°ê³¼ ê°œìš”ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    responses = {}
    
    for category, activity in zip(selected_categories, activities):
        
        prompt = f"""
        ë‹¹ì‹ ì€ ìê¸°ì†Œê°œì„œ ê¸€ê° ë° ê°œìš” ì¶”ì¶œì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” AI ì¡°ë ¥ìì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ì œê³µí•œ ì…ë ¥ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ ì§€ì¹¨ì— ë”°ë¼ ìê¸°ì†Œê°œì„œ ê¸€ê°ê³¼ ê°œìš”ë¥¼ ìƒì„±í•˜ì„¸ìš”.

        1. **ì§ë¬´ ì •ë³´**:
           - ì§ë¬´ ì´ë¦„: {job}
           - ì´ ì§ë¬´ì™€ ê´€ë ¨ëœ í•„ìš” ì—­ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {skills}

        2. **ì‚¬ìš©ì í™œë™ ì •ë³´**:
           - **{category}**: {activity}

        3. **ìš”ì²­ ì‚¬í•­**:
           - {skills} ì¤‘ í•´ë‹¹ í™œë™ì„ í†µí•´ ê°•ì¡°í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì„ í•˜ë‚˜ë§Œ ê³¨ë¼ ê¸€ê°ê³¼ ê°œìš”ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
           - ê¸€ê°: ì…ë ¥í•œ í™œë™ì—ì„œ ë„ì¶œëœ ì£¼ìš” ì£¼ì œë¥¼ ê°„ê²°íˆ í‘œí˜„í•˜ì„¸ìš”.
           - ê°œìš”: ê¸€ê°ì—ì„œ ë„ì¶œëœ ì£¼ì œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ìê¸°ì†Œê°œì„œì—ì„œ í™œìš© ê°€ëŠ¥í•œ ì„¸ë¶€ ë‚´ìš©ì„ í¬í•¨í•˜ì„¸ìš”.
             - **ë°°ê²½ ì„¤ëª…**: [í™œë™ì˜ ë°°ê²½ ë° ë§¥ë½]
             - **ì„±ê³¼/ê²°ê³¼**: [í™œë™ì˜ ê²°ê³¼ë‚˜ ì„±ì·¨]
             - **ì§ë¬´/ê¸°ì—…ê³¼ì˜ ì—°ê²°**: [ì§ë¬´ë‚˜ ê¸°ì—…ì˜ ìš”êµ¬ ì‚¬í•­ì— ëŒ€í•œ ì—°ê´€ì„±]

        4. **ì¶œë ¥ í˜•ì‹**:
           - **ê°•ì¡° ì—­ëŸ‰**: [ê°•ì¡°í•  ì—­ëŸ‰]
             **ê¸€ê°**: [ì‚¬ìš©ìì˜ í™œë™ì—ì„œ ì¶”ì¶œëœ ì£¼ìš” ì£¼ì œ]
             **ê°œìš”**:
               - **ë°°ê²½ ì„¤ëª…**: [í™œë™ì˜ ë°°ê²½ ë° ë§¥ë½]
               - **ì„±ê³¼/ê²°ê³¼**: [í™œë™ì˜ ê²°ê³¼ë‚˜ ì„±ì·¨]
               - **ì§ë¬´/ê¸°ì—…ê³¼ì˜ ì—°ê²°**: [ì§ë¬´ë‚˜ ê¸°ì—…ì˜ ìš”êµ¬ ì‚¬í•­ì— ëŒ€í•œ ì—°ê´€ì„±]
        """

        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1024,
                temperature=0.3
            )
            responses[category] = response.choices[0].message.content.strip()
        except Exception as e:
            responses[category] = f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    return responses



def generate_q1(job, personal_statement):
    
    prompt = f"""
ë‹¹ì‹ ì€ í™œë™ ê¸°ë°˜ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ëŠ” ì§€ì›ìê°€ ì…ë ¥í•œ í™œë™ì„ ê¸°ë°˜ìœ¼ë¡œ í•„ìš”í•œ ì—­ëŸ‰ì„ ê°•ì¡°í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±ëœ ìê¸°ì†Œê°œì„œ ê¸€ê° ê°œìš”ì…ë‹ˆë‹¤. 
ì´ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™œë™ ê¸°ë°˜ ë©´ì ‘ ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”. 
ê° ì§ˆë¬¸ì€ {job} ì§ë¬´ì— ì§€ì›í•˜ëŠ” ì§€ì›ìì˜ ê²½í—˜ê³¼ ì—­ëŸ‰ì„ íš¨ê³¼ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. 
ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:

1. ì²« ë²ˆì§¸ ì§ˆë¬¸ì€ í…ìŠ¤íŠ¸ì—ì„œ ì–¸ê¸‰ëœ ì£¼ìš” í™œë™ì´ë‚˜ ê²½í—˜ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ íƒêµ¬í•˜ëŠ” ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
2. ë‘ ë²ˆì§¸ ì§ˆë¬¸ì€ ì§€ì›ìì˜ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥, í˜‘ì—… ê²½í—˜, ë˜ëŠ” ì˜ì‚¬ê²°ì • ê³¼ì •ì„ í‰ê°€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
3. ì„¸ ë²ˆì§¸ ì§ˆë¬¸ì€ ì§€ì›ìì˜ í•´ë‹¹ ê²½í—˜ì—ì„œ ì–»ì€ êµí›ˆì´ë‚˜ ë°°ìš´ ì ì„ ì§ë¬´ì™€ ì—°ê²° ì§€ì„ ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ì…ë ¥ í…ìŠ¤íŠ¸:
"{personal_statement}"

ì¶œë ¥ (í™œë™ ê¸°ë°˜ ë©´ì ‘ ì§ˆë¬¸):
1. [ì£¼ìš” í™œë™ì´ë‚˜ ê²½í—˜ì„ íƒêµ¬í•˜ëŠ” ì§ˆë¬¸]
2. [ë¬¸ì œ í•´ê²°, í˜‘ì—…, ì˜ì‚¬ê²°ì •ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸]
3. [ê²½í—˜ê³¼ ì§ë¬´ ê´€ë ¨ ì—­ëŸ‰ì„ ì—°ê²°í•˜ëŠ” ì§ˆë¬¸]
"""
    
    # OpenAI API í˜¸ì¶œ
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            temperature=0
        )
        # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"



###############################################################################
# ë‰´ìŠ¤ í‚¤ì›Œë“œ ë½‘ëŠ” í•¨ìˆ˜
###############################################################################

def generate_news_keyword(job, company):
    prompt = f"""
ë‹¹ì‹ ì€ íŠ¸ë Œë“œ ê´€ë ¨ í‚¤ì›Œë“œ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì–´ë–¤ ì§ë¬´ì™€ íšŒì‚¬ê°€ ì£¼ì–´ì§€ë”ë¼ë„, ê´€ë ¨ëœ 2025ë…„ ìµœì‹  íŠ¸ë Œë“œ, ê¸°ìˆ , í˜¹ì€ ì‚¬ë¡€ë¥¼ ì¡°ì‚¬í•  ë•Œ ìœ ìš©í•œ **êµ¬ì²´ì ì´ê³  ì„¸ë¶€ì ì¸ í‚¤ì›Œë“œ** 3ê°œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
'{company}'ì˜ '{job}' ì§ë¬´ì— ê´€ë ¨ëœ íŠ¸ë Œë“œ ì •ë³´ ë° ë°°ê²½ ì§€ì‹ì„ ì¡°ì‚¬í•  ë•Œ ìœ ìš©í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ” **êµ¬ì²´ì ì´ê³  ì„¸ë¶€ì ì¸ í‚¤ì›Œë“œ** 3ê°œë¥¼ ë„ì¶œí•˜ì„¸ìš”.

í‚¤ì›Œë“œëŠ” ë‹¤ìŒ ì¡°ê±´ì„ ì¶©ì¡±í•´ì•¼ í•©ë‹ˆë‹¤:
1. '{job}' ì§ë¬´ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ëœ 2025ë…„ ìµœì‹  íŠ¸ë Œë“œ, ê¸°ìˆ , í˜¹ì€ ì‚¬ë¡€ë¥¼ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
2. ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë‚˜ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ê²€ìƒ‰í–ˆì„ ë•Œ, **êµ¬ì²´ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ë°”ë¡œ ì°¾ì„ ìˆ˜ ìˆëŠ” í˜•íƒœ**ì—¬ì•¼ í•©ë‹ˆë‹¤.
3. í‚¤ì›Œë“œëŠ” íŠ¹ì • í™œë™, ê¸°ìˆ , í˜¹ì€ íŠ¸ë Œë“œì²˜ëŸ¼ **ëª…í™•í•˜ê³  ì„¸ë¶€ì ì¸ ì£¼ì œ**ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
4. í‚¤ì›Œë“œëŠ” **ëª…ì‚¬ + ëª…ì‚¬** í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•˜ë©°, "~ì˜" ê°™ì€ ì¡°ì‚¬ë‚˜ ë¶ˆí•„ìš”í•œ ì ‘ì†ì‚¬ë¥¼ í¬í•¨í•˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤.
5. ì…ë ¥ëœ ì§ë¬´ë‚˜ íšŒì‚¬ì˜ **ì‚°ì—… ë„ë©”ì¸ íŠ¹ì„±**ì„ ë°˜ì˜í•˜ì—¬, ì¼ë°˜ì ì´ì§€ ì•Šê³  ê´€ë ¨ì„±ì´ ë†’ì€ í‚¤ì›Œë“œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹:
- í‚¤ì›Œë“œ 1: [êµ¬ì²´ì ì¸ ê¸°ìˆ /í™œë™/íŠ¸ë Œë“œ]
- í‚¤ì›Œë“œ 2: [êµ¬ì²´ì ì¸ ê¸°ìˆ /í™œë™/íŠ¸ë Œë“œ]
- í‚¤ì›Œë“œ 3: [êµ¬ì²´ì ì¸ ê¸°ìˆ /í™œë™/íŠ¸ë Œë“œ]

"""
    # OpenAI API í˜¸ì¶œ
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            temperature=0.8
        )
        # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
        content = response.choices[0].message.content.strip()

        # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        keywords = []
        for line in content.split("\n"):
            if line.startswith("- í‚¤ì›Œë“œ"):
                keyword = line.split(":")[1].strip()
                keyword = keyword.strip('"').strip("'")
                keywords.append(keyword)

        return keywords
    except Exception as e:
        return f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
# ë‰´ìŠ¤ í‚¤ì›Œë“œ í•¨ìˆ˜ í˜¸ì¶œ ë° ì¶œë ¥

###############################################################################
# ë‰´ìŠ¤ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
###############################################################################
import requests

def search_news_by_keyword(keyword):

    # API URL ë° íŒŒë¼ë¯¸í„°
    url = "https://api-v2.deepsearch.com/v1/articles"
    params = {
        "keyword": keyword,
        # "date_from": "2024-12-01",
        "page_size": 30,
        "api_key": ""
    }

    # API ìš”ì²­
    response = requests.get(url, params=params)

    # ì‘ë‹µ ì²˜ë¦¬
    if response.status_code == 200:
        # JSON ë°ì´í„° íŒŒì‹±
        data = response.json()

        # ë°ì´í„°ì—ì„œ í•„ìš”í•œ í•­ëª©ë§Œ ì¶”ì¶œ
        articles = data.get("data", [])
        extracted_data = []
        for article in articles:
            extracted_data.append({
                "Title": article.get("title", "N/A"),
                "Date": article.get("published_at", "N/A").split("T")[0] if article.get("published_at") else "N/A",
                "Section": ", ".join(article.get("sections", ["N/A"])),
                "Publisher": article.get("publisher", "N/A"),
                "Summary": article.get("summary", "N/A").replace("\n", " "),
                "Content URL": article.get("content_url", "N/A")
            })

        # DataFrame ìƒì„±
        news_df = pd.DataFrame(extracted_data)
        return news_df
    else:
        print(f"API í˜¸ì¶œ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
        print(response.text)
        return pd.DataFrame()  # ë¹ˆ DataFrame ë°˜í™˜
    

# Kiwi ì´ˆê¸°í™”
kiwi = Kiwi()
stopwords_dict = Stopwords()

# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def Kr_preprocessing(text):
    text = text.strip()
    text = re.sub(r'[^\d\s\w]', ' ', text)
    kiwi_tokens = kiwi.tokenize(text, stopwords=stopwords_dict)
    noun_words = [token.form for token in kiwi_tokens if 'NN' in token.tag and len(token.form) > 1]
    return ' '.join(noun_words)

def cluster_news(news):

    # ë‰´ìŠ¤ ìš”ì•½ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    total_docs = []
    for i in range(len(news)):
        total_docs.append(news.loc[i, 'Summary'])

    # ì „ì²˜ë¦¬ ìˆ˜í–‰
    filtered_docs = [Kr_preprocessing(doc) for doc in total_docs]

    # TF-IDF ê¸°ë°˜ DTM ìƒì„±
    tfidf_vectorizer = TfidfVectorizer()
    DTM_tfidf = tfidf_vectorizer.fit_transform(filtered_docs)
    DTM_TFIDF = np.array(DTM_tfidf.todense())

    # PCA ìˆ˜í–‰ (8ê°œì˜ ì£¼ì„±ë¶„)
    pca = PCA(n_components=8)
    pca_results_tfidf = pca.fit_transform(DTM_TFIDF)
    
    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê¸°ë°˜ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„ íƒ
    best_n_clusters = 0
    best_score = -1
    scores = []  # ëª¨ë“  n_clustersì— ëŒ€í•œ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ì €ì¥

    # n_clusters ë²”ìœ„ë¥¼ ì§€ì • (2ë¶€í„° len(filtered_docs)ê¹Œì§€)
    for i in range(2, len(filtered_docs)):
        kmeans = KMeans(n_clusters=i, random_state=42)
        cluster_pca_ifidf = kmeans.fit_predict(pca_results_tfidf)

        # í˜„ì¬ n_clustersì— ëŒ€í•œ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
        score = silhouette_score(pca_results_tfidf, cluster_pca_ifidf)
        scores.append(score)

        # ìµœê³  ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ì™€ í•´ë‹¹ n_clusters ì—…ë°ì´íŠ¸
        if score > best_score:
            best_score = score
            best_n_clusters = i

    # ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ KMeans ì‹¤í–‰
    kmeans = KMeans(n_clusters=best_n_clusters, random_state=42)
    final_clusters = kmeans.fit_predict(pca_results_tfidf)

    news['cluster_id'] = [cluster_id + 1 for cluster_id in final_clusters]
    news = news.sort_values(by='cluster_id').reset_index(drop=True)

    cluster_dict = {
        cluster_id: cluster_df.reset_index(drop=True)
        for cluster_id, cluster_df in news.groupby('cluster_id')
    }

    return cluster_dict


def create_cluster_text(clustered_news):
    cluster_summaries = {}

    for key, value in clustered_news.items():
        summaries = value['Summary'].tolist()
        cluster_summaries[key] = summaries

    # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
    output_string = []

    for cluster_id, summaries in cluster_summaries.items():
        output_string.append(f"Cluster {cluster_id}")
        output_string.extend(summaries)
        output_string.append("")

    return "\n".join(output_string)


# ë‰´ìŠ¤ ì„ ì • í•¨ìˆ˜

def summarize_cluster(text, job, company, keyword):
    prompt = f"""
ë‹¹ì‹ ì€ ì§ë¬´ ê´€ë ¨ íŠ¸ë Œë“œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì•„ë˜ textëŠ” {company} íšŒì‚¬ì˜ {job} ì§ë¬´ì— ì§€ì›í•˜ëŠ” ì§€ì›ìê°€ ë©´ì ‘ ì¤€ë¹„ ê³¼ì •ì—ì„œ {keyword}ë¥¼ ê²€ìƒ‰ì–´ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤.
ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜ë˜ì–´ ìˆëŠ”ë°, ì´ë•Œ ê° í´ëŸ¬ìŠ¤í„°ëŠ” íŠ¹ì • ì£¼ì œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±ëœ ê¸°ì‚¬ì˜ ìš”ì•½ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. 
ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° í´ëŸ¬ìŠ¤í„°ì˜ **ì£¼ì œ**ë¥¼ ë„ì¶œí•˜ê³ , í´ëŸ¬ìŠ¤í„° ë‚´ ë¬¸ì¥ë“¤ì„ ë¶„ì„í•˜ì—¬ **ì ë‹¹í•œ ê¸¸ì´ì˜ í•µì‹¬ ìš”ì•½**ì„ ì‘ì„±í•˜ì„¸ìš”.

ì¶œë ¥ ì‹œ ê°€ì¥ ì¤‘ìš”í•œ í´ëŸ¬ìŠ¤í„° top 3ë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì •ë ¬ ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
1. (ê°€ì¥ ì¤‘ìš”) ì§€ì›ìì˜ {job} ì±„ìš© ì¤€ë¹„ ê³¼ì •ì—ì„œ **ì§ë¬´ íŠ¸ë Œë“œë¥¼ ê¹Šì´ ì´í•´í•˜ëŠ” ë° ì‹¤ì§ˆì ì¸ ë„ì›€ì„ ì£¼ëŠ”ê°€** (40%)
2. {keyword}ì™€ì˜ ì—°ê´€ì„±ì´ ë†’ìœ¼ë©°, ì§ë¬´ ìˆ˜í–‰ì— ìˆì–´ í•„ìˆ˜ì ì¸ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ê°€ (30%)
3. ì£¼ì œê°€ íŠ¹ì • ê¸°ì—…ì˜ í™ë³´ì— ì¹˜ìš°ì¹˜ì§€ ì•Šê³ , ì—…ê³„ ì „ë°˜ì˜ íë¦„ì„ í¬ê´„í•˜ë©° ë‹¤ì–‘í•œ ì‹œê°ì„ ì œê³µí•˜ëŠ”ê°€ (30%)


ì˜ˆì‹œë¡œ, ë‹¤ìŒê³¼ ê°™ì´ í´ëŸ¬ìŠ¤í„°ë¥¼ ì„ ì •í•´ì•¼ í•©ë‹ˆë‹¤.

â€˜ë””ì§€í„¸ ê¸ˆìœµì˜ ë°œì „ê³¼ í•€í…Œí¬ ì‚°ì—… ì„±ì¥â€™ (ì ì ˆí•œ í´ëŸ¬ìŠ¤í„°)
: í•€í…Œí¬ ë° ë””ì§€í„¸ ê¸ˆìœµ íŠ¸ë Œë“œ, ê¸°ì—…ë“¤ì˜ íˆ¬ì ë° í˜ì‹  ì‚¬ë¡€ê°€ í¬í•¨ë˜ì–´ ìˆì–´ ê¸ˆìœµ ë¶„ì•¼ì˜ ë³€í™” íë¦„ì„ ì´í•´í•˜ëŠ” ë° ì í•©í•¨.

â€˜ê¸€ë¡œë²Œ ê²½ê¸° ë‘”í™”ì™€ êµ­ë‚´ ê¸ˆìœµ ì •ì±… ëŒ€ì‘â€™ (ì ì ˆí•œ í´ëŸ¬ìŠ¤í„°)
: ê²½ì œ í™˜ê²½ ë³€í™”ì— ë”°ë¥¸ ì •ì±…ì  ëŒ€ì‘ ì „ëµì„ í¬í•¨í•˜ê³  ìˆì–´ ì§€ì›ìê°€ ê²½ì œ ì „ë°˜ì˜ ë§¥ë½ì„ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŒ.

â€˜ì€í–‰ë³„ ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ ì¶œì‹œ ì†Œì‹â€™ (ë¶€ì ì ˆí•œ í´ëŸ¬ìŠ¤í„°)
: íŠ¹ì • ì€í–‰ì˜ ê°œë³„ ì„œë¹„ìŠ¤ í™ë³´ì— ì´ˆì ì´ ë§ì¶°ì ¸ ìˆìœ¼ë©°, ì‚°ì—… ì „ì²´ì˜ íë¦„ì„ ì´í•´í•˜ëŠ” ë°ëŠ” í•œê³„ê°€ ìˆìŒ.


**ì¶œë ¥ í˜•ì‹**:
ìµœì¢… í´ëŸ¬ìŠ¤í„°: a, b, c
1. [í´ëŸ¬ìŠ¤í„° ì£¼ì œ]
: [í´ëŸ¬ìŠ¤í„° aì˜ ìš”ì•½ë¬¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ í•µì‹¬ ìš”ì•½]

2. [í´ëŸ¬ìŠ¤í„° ì£¼ì œ]
: [í´ëŸ¬ìŠ¤í„° bì˜ ìš”ì•½ë¬¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ í•µì‹¬ ìš”ì•½]

3. [í´ëŸ¬ìŠ¤í„° ì£¼ì œ]
: [í´ëŸ¬ìŠ¤í„° cì˜ ìš”ì•½ë¬¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ëœ í•µì‹¬ ìš”ì•½]
...

**ì…ë ¥ í…ìŠ¤íŠ¸**:
{text}

**ì¶œë ¥ ì§€ì¹¨**:
- í´ëŸ¬ìŠ¤í„° a, b, cì—ëŠ” ê°ê° í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ idê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.
- ê° í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ì œëŠ” í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ë¥¼ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” ë‹¨ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
- í•µì‹¬ ìš”ì•½ì€ í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ëœ ë¬¸ì¥ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì¼ê´€ë˜ê²Œ ì‘ì„±í•˜ë˜, ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë¹ ëœ¨ë¦¬ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.
- ìš”ì•½ì€ í´ëŸ¬ìŠ¤í„° ë‚´ ë¬¸ì¥ë“¤ì˜ ì£¼ìš” ë‚´ìš©ì„ ì¢…í•©í•œ í˜•íƒœë¡œ ì‘ì„±í•˜ë©°, ì§€ë‚˜ì¹˜ê²Œ ì„¸ë¶€ì ì´ê±°ë‚˜ ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì œì™¸í•©ë‹ˆë‹¤.

...

**ì¶œë ¥ ì˜ˆì‹œ**:
1. êµ­ë‚´ AI ì°½ì—…ìì˜ ê¸€ë¡œë²Œ ì˜í–¥ë ¥ ë° ì„±ê³µ ì‚¬ë¡€
: í•œêµ­ AI ì°½ì—…ìë“¤ì´ ê¸€ë¡œë²Œ ì‹œì¥ì—ì„œ ë‘ê°ì„ ë‚˜íƒ€ë‚´ë©° ì„±ê³µì ì¸ íˆ¬ì ìœ ì¹˜ì™€ í˜ì‹ ì ì¸ AI ì†”ë£¨ì…˜ì„ ì œê³µí•˜ê³  ìˆìŒ. 
í¬ë¸ŒìŠ¤ê°€ ì„ ì •í•œ 'ì£¼ëª©í•  AI ì°½ì—…ì'ë¡œ ì†Œê°œë˜ë©° ê¸€ë¡œë²Œ AI ì‚°ì—…ì—ì„œì˜ ì…ì§€ë¥¼ ê°•í™”í•¨.

"""

    # OpenAI API í˜¸ì¶œ
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            temperature=0
        )
        # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
        content = response.choices[0].message.content.strip()
        
        return content
    except Exception as e:
        return f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    

def generate_q2(job, company, k_idx, topic, text):
    """
    ìš”ì•½ëœ ë‰´ìŠ¤ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±

    """
    # í”„ë¡¬í”„íŠ¸ ì‘ì„±
    prompt = (f"""
    ì•„ë˜ textëŠ” {company} íšŒì‚¬ì˜ {job} ì§ë¬´ì— ì§€ì›í•˜ëŠ” ì§€ì›ìê°€ ë©´ì ‘ ì¤€ë¹„ ê³¼ì •ì—ì„œ {news_keywords[k_idx-1]}ë¥¼ ê²€ìƒ‰ì–´ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•œ í›„,
    ìì‹ ì´ ì›í•˜ëŠ” ì£¼ì œì˜ ê¸°ì‚¬ë§Œ ê³¨ë¼ì„œ ìš”ì•½í•œ ê²°ê³¼ì…ë‹ˆë‹¤. 
    ì•„ë˜ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{job}' ì§ë¬´ë¥¼ ì¤€ë¹„í•˜ëŠ” ì§€ì›ìì—ê²Œ ë„ì›€ì´ ë  ë§Œí•œ ì§ˆë¬¸ 5ê°œë¥¼ ìƒì„±í•˜ê³ , ê° ì§ˆë¬¸ì„ í‰ê°€í•œ ë’¤, Top 3ê°œì˜ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

    **ì…ë ¥ ë‚´ìš©**:
    ë‰´ìŠ¤ ì£¼ì œ: {topic}
    ìš”ì•½ ë‚´ìš©: {text}
        
    **ì§ˆë¬¸ ìƒì„± ë° í‰ê°€ ì§€ì¹¨**
    [ì§ˆë¬¸ ìƒì„±]
        - ë„ë©”ì¸ ê´€ì‹¬ë„ í‰ê°€: ê¸°ì‚¬ ë‚´ìš© ë° ê´€ë ¨ íŠ¸ë Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì›ìê°€ í•´ë‹¹ ë„ë©”ì¸ì— ëŒ€í•œ ê´€ì‹¬ê³¼ ì´í•´ë„ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.
        - ìƒí™© ê¸°ë°˜ ì§ˆë¬¸: ê¸°ì‚¬ì—ì„œ ë‹¤ë£¨ëŠ” íŠ¹ì • ìƒí™©ì„ ë°”íƒ•ìœ¼ë¡œ ì§€ì›ìì˜ ì‚¬ê³ ë ¥ê³¼ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í‰ê°€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.
        - í† ë¡  ìœ ë„ ì§ˆë¬¸: ê¸°ì‚¬ì—ì„œ ì–¸ê¸‰ëœ ì‚°ì—… ë™í–¥, ê²½ìŸ ìƒí™©, ë˜ëŠ” ì†Œë¹„ì í–‰ë™ ë³€í™”ì™€ ê´€ë ¨í•˜ì—¬ ì§€ì›ìì˜ ì˜ê²¬ì„ ìœ ë„í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.
        - ì°½ì˜ì  ì‚¬ê³  ì§ˆë¬¸: ê¸°ì‚¬ì—ì„œ ë‹¤ë£¬ ì£¼ì œë¥¼ í™•ì¥í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•  ìˆ˜ ìˆë„ë¡ ì°½ì˜ì ì¸ ì‚¬ê³ ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.

    [ì§ˆë¬¸ í‰ê°€]
      - ë„ë©”ì¸ ê´€ë ¨ ì§€ì‹ í‰ê°€ (ìµœëŒ€ 5ì ): '{job}' ì§ë¬´ì™€ ê´€ë ¨ëœ ë„ë©”ì¸ì˜ {news_keywords[k_idx-1]} ì§€ì‹ì„ í‰ê°€í•  ìˆ˜ ìˆì–´ì•¼ í•¨.
      - ì§ë¬´ ìˆ˜í–‰ì— í•„ìš”í•œ ì´í•´ë„ í‰ê°€ (ìµœëŒ€ 5ì ): ì§ë¬´ ìˆ˜í–‰ì— í•„ìš”í•œ ë„ë©”ì¸ ì´í•´ë„ë¥¼ í‰ê°€í•  ìˆ˜ ìˆì–´ì•¼ í•¨.
      - ì§ˆë¬¸ì˜ ì ì ˆì„± ë° ë²”ìš©ì„± (ìµœëŒ€ 3ì ): íŠ¹ì • ê¸°ì—…ì´ë‚˜ ê¸°ìˆ ì— ì¹˜ìš°ì¹˜ì§€ ì•Šìœ¼ë©° ë³´í¸ì ìœ¼ë¡œ í‰ê°€ ê°€ëŠ¥í•´ì•¼ í•¨. ì§€ì—½ì ì¼ ê²½ìš° ê°ì (-3ì ).
      - ì°½ì˜ì  ì‚¬ê³  ìœ ë„ (ìµœëŒ€ 3ì ): ì°½ì˜ì  ì‚¬ê³ ë¥¼ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì¼ ê²½ìš° ë†’ì€ ì ìˆ˜.
      - ì§€ë‚˜ì¹˜ê²Œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ ë°©ì§€ (-2ì ): ì§€ë‚˜ì¹˜ê²Œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì€ ê°ì  ì²˜ë¦¬.

    

    **ì¶œë ¥ í˜•ì‹**
    ìœ„ì˜ ì§ˆë¬¸ ìƒì„± ë° í‰ê°€ ì§€ì¹¨ì„ í™œìš©í•˜ì—¬, ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ 3ê°œì˜ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    ì´ë•Œ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ê³ , ì ìˆ˜ ë“± í‰ê°€ ë‚´ìš©ì€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
       1. [ì§ˆë¬¸ ë‚´ìš©]
       2. [ì§ˆë¬¸ ë‚´ìš©]
       3. [ì§ˆë¬¸ ë‚´ìš©]
    """
)

    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",  
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            temperature=0
        )
        
        # ì‘ë‹µì—ì„œ ì§ˆë¬¸ ì¶”ì¶œ
        generated_questions = response.choices[0].message.content.strip()
        questions = [q.strip() for q in generated_questions.split("\n") if q.strip()]
        
        # 4ê°œ ì§ˆë¬¸ë§Œ ë°˜í™˜
        return questions
        #return generated_questions
    
    except Exception as e:
        return f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        


kiwi = Kiwi()
stopwords_dict = Stopwords()

def Kr_preprocessing2(text):
    custom_stopwords = ['í† ìŠ¤', 'ì„œë¹„ìŠ¤', 'ê²½í—˜', 'ë¬¸ì œ', 'ì—…ë¬´', 'í•„ìš”', 'ê´€ë ¨', 'ê¸°ìˆ ', 
                        'ë‹¤ì–‘', 'í•´ê²°', 'ì´í•´', 'ì œí’ˆ', 'ë³´ìœ ', 'ì‘ì„±', 'ì´ìƒ', 'ê³¼ì •', 'ì£¼ë„',
                        'í™œìš©', 'ì¤‘ìš”', 'ëŠ¥ë ¥', 'ì¡°ì§', 'ì œì•ˆ']
    for word in custom_stopwords:
        stopwords_dict.add((word, 'NNG'))
    
    text = text.strip()
    text = re.sub(r'[^\d\s\w]', ' ', text)
    
    kiwi_tokens = kiwi.tokenize(text, stopwords=stopwords_dict)
    noun_words = [token.form for token in kiwi_tokens if 'NN' in token.tag and len(token.form) > 1]
    return noun_words


def extract_keywords(job_posting, max_keywords=10):
    
    processed_data = Kr_preprocessing2(job_posting)
    
    vectorizer = TfidfVectorizer(max_features=max_keywords)
    tfidf_matrix = vectorizer.fit_transform(processed_data)
    keywords = vectorizer.get_feature_names_out()

    return list(keywords)


def generate_q3(job, keywords, job_posting):
    """
    LLMì„ ì´ìš©í•˜ì—¬ ì§ë¬´ì™€ ê´€ë ¨ëœ ì§€ì‹ ì¤‘ì‹¬ì˜ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    prompt = f"""
    ë‹¹ì‹ ì€ ì§ë¬´ ê´€ë ¨ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ì§ë¬´ì™€ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë©´ì ‘ ì§ˆë¬¸ì—ì„œ ì‚¬ìš©ìì˜ íŠ¹ì • ì§€ì‹ì´ë‚˜ ê¸°ìˆ  ì´í•´ë„ë¥¼ í‰ê°€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.
    ì§ˆë¬¸ì€ ë‹¤ìŒ ì¡°ê±´ì„ ì¶©ì¡±í•´ì•¼ í•©ë‹ˆë‹¤:
    1. ì§ˆë¬¸ì€ "{job}" ì§ë¬´ì—ì„œ í•„ìš”í•œ êµ¬ì²´ì ì¸ ì§€ì‹(ì˜ˆ: ê¸°ìˆ , ê°œë…, ì´ë¡ )ì— ëŒ€í•´ ë¬¼ì–´ì•¼ í•©ë‹ˆë‹¤.
    2. ì§ˆë¬¸ í˜•ì‹ì€ ê°„ë‹¨í•˜ë©°, ì˜ˆë¥¼ ë“¤ì–´ "{job}"ì™€ ê´€ë ¨ëœ íŠ¹ì • ê°œë…ì´ë‚˜ ê¸°ìˆ ì— ëŒ€í•´ ì„¤ëª…ì„ ìš”ì²­í•˜ëŠ” í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
    3. ì±„ìš© ê³µê³ ì¸ '{job_posting}'ì„ 1ìˆœìœ„ë¡œ ì°¸ê³ í•˜ê³ , '{keywords}'ëŠ” 2ìˆœìœ„ë¡œ ì°¸ê³ í•˜ì„¸ìš”.

    ì˜ˆì‹œ:
    ì§ë¬´: "ë°ì´í„° ë¶„ì„ê°€"
    ì¶”ì¶œëœ í‚¤ì›Œë“œ: "ë°ì´í„°, ë¶„ì„, ë¬¸ì œ í•´ê²°"
    ì¶œë ¥ (ë©´ì ‘ ì§ˆë¬¸):
    1. ë°ì´í„° ë¶„ì„ì—ì„œ 'hierarchical clustering'ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?
    2. 'k-means clustering'ì˜ ì‘ë™ ë°©ì‹ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    3. ë°ì´í„° ë¶„ì„ ê³¼ì •ì—ì„œ 'PCA(ì£¼ì„±ë¶„ ë¶„ì„)'ê°€ ì‚¬ìš©ë˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?


    ì´ì œ ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

    ì§ë¬´: "{job}"
    ì±„ìš© ê³µê³ : "{job_posting}"
    ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}

    ì¶œë ¥ (ë©´ì ‘ ì§ˆë¬¸ 3ê°œ):
    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a professional interview question generator."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=300
    )

    # ì‘ë‹µì—ì„œ ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ
    content = response.choices[0].message.content
    return content




####################
#### streamlit code
####################

import streamlit as st

st.set_page_config(
    page_title="ë°”ì˜ë‹¤ ë°”ë¹  ì·¨ì¤€ìƒì„ ìœ„í•œ AI Agent",  # í˜ì´ì§€ ì œëª©
    page_icon="ğŸ—‚ï¸")          


# Step 1: ì§ë¬´ ë° íšŒì‚¬ ì…ë ¥
st.header("1. ì§ë¬´ ë° íšŒì‚¬ ì…ë ¥")
user_job = st.text_input("ì§ë¬´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
user_company = st.text_input("íšŒì‚¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
user_full_job = f"{user_company} {user_job}"

if st.button("ì±„ìš© ê³µê³  ìƒì„±"):
    if user_job and user_company:
        user_job_posting = generate_job_posting(user_job, user_company)
        st.session_state["job_posting"] = user_job_posting
        st.success("ì±„ìš© ê³µê³ ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.text_area("ìƒì„±ëœ ì±„ìš© ê³µê³ ", user_job_posting, height=300)
    else:
        st.error("ì§ë¬´ì™€ íšŒì‚¬ë¥¼ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")

# Step 2: í•„ìš” ì—­ëŸ‰ ì¶”ì¶œ
st.header("2. í•„ìš” ì—­ëŸ‰ ì¶”ì¶œ")
if "job_posting" in st.session_state:
    similar_sum = db.loc[retrieval(st.session_state["job_posting"], 3), 'total_sum'].tolist()
    user_skills = get_required_skills(user_job, st.session_state["job_posting"], similar_sum)
    st.session_state["skills"] = user_skills
    st.success("í•„ìš” ì—­ëŸ‰ì´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤")
    st.text_area("ì¶”ì¶œëœ í•„ìš” ì—­ëŸ‰", user_skills, height=500)

# Step 3: ìì†Œì„œ ê¸€ê° ìƒì„±
st.header("3. ìê¸°ì†Œê°œì„œ ê¸€ê° ìƒì„±")
categories = ["ë™ê¸°/í¬ë¶€", "ì„±ì¥/ê°€ì¹˜ê´€", "ì—­ëŸ‰/ê²½í—˜", "í˜‘ì—…/ì„±ê³¼", "ê¸°ì—…/ì•„ì´ë””ì–´"]
selected_categories = st.multiselect("ë„ì›€ì´ í•„ìš”í•œ ë¬¸í•­ì„ ì„ íƒí•˜ì„¸ìš”", categories)

if selected_categories:
    activities = []
    for category in selected_categories:
        activity = st.text_area(f"'{category}'ì— ëŒ€í•œ í™œë™ì„ ì…ë ¥í•˜ì„¸ìš”:", height=200)
        activities.append(activity)
    
    if st.button("ìê¸°ì†Œê°œì„œ ê¸€ê° ë° ê°œìš” ìƒì„±"):
        if "job_posting" in st.session_state:
            job_posting = st.session_state["job_posting"]
            user_personal_statement = generate_personal_statement(user_job, selected_categories, activities, user_skills)
            st.session_state["personal_statement"] = user_personal_statement
            st.success("ìê¸°ì†Œê°œì„œ ê¸€ê° ë° ê°œìš”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            for category, content in user_personal_statement.items():
                st.subheader(category)
                st.write(content)
        else:
            st.error("ë¨¼ì € ì±„ìš© ê³µê³ ë¥¼ ìƒì„±í•˜ì„¸ìš”.")

# Step 4: ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
st.header("4. ë©´ì ‘ ì§ˆë¬¸ ìƒì„±")

if "personal_statement" in st.session_state:
    tab1, tab2, tab3 = st.tabs(["í™œë™ ê¸°ë°˜ ì§ˆë¬¸", "ë‰´ìŠ¤ íŠ¸ë Œë“œ ì§ˆë¬¸", "ë‹¨ìˆœ ê¸°ìˆ  ì§ˆë¬¸"])

    with tab1:
        st.subheader("í™œë™ ê¸°ë°˜ ì§ˆë¬¸")
        user_q1 = generate_q1(user_job, activities)
        st.write(user_q1)

    with tab2:
        st.subheader("ë‰´ìŠ¤ íŠ¸ë Œë“œ ì§ˆë¬¸")

        # ì„¸ì…˜ ìƒíƒœì—ì„œ ê¸°ì¡´ ë‰´ìŠ¤ í‚¤ì›Œë“œë¥¼ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìƒì„±
        if "news_keywords" not in st.session_state:
            st.session_state["news_keywords"] = generate_news_keyword(user_job, user_company)

        news_keywords = st.session_state["news_keywords"]

        # ì„¸ì…˜ ìƒíƒœì—ì„œ ë‰´ìŠ¤ ë°ì´í„° í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ê²€ìƒ‰
        if "news_data" not in st.session_state:
            dataframes = {}
            for idx, keyword in enumerate(news_keywords, start=1):
                dataframe_name = f"news{idx}"
                dataframes[dataframe_name] = search_news_by_keyword(keyword)

            # ë‰´ìŠ¤ ë°ì´í„° ì €ì¥
            st.session_state["news_data"] = {
                "news1": dataframes['news1'],
                "news2": dataframes['news2'],
                "news3": dataframes['news3']
            }

        # ì„¸ì…˜ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
        news1 = st.session_state["news_data"]["news1"]
        news2 = st.session_state["news_data"]["news2"]
        news3 = st.session_state["news_data"]["news3"]

        # ë‰´ìŠ¤ í´ëŸ¬ìŠ¤í„° ë° í…ìŠ¤íŠ¸ ìš”ì•½
        if "news_summaries" not in st.session_state:
            st.session_state["news_summaries"] = {
                "news_dict1": cluster_news(news1),
                "news_dict2": cluster_news(news2),
                "news_dict3": cluster_news(news3),
                "cluster_text1": create_cluster_text(cluster_news(news1)),
                "cluster_text2": create_cluster_text(cluster_news(news2)),
                "cluster_text3": create_cluster_text(cluster_news(news3))
            }

        cluster_text1 = st.session_state["news_summaries"]["cluster_text1"]
        cluster_text2 = st.session_state["news_summaries"]["cluster_text2"]
        cluster_text3 = st.session_state["news_summaries"]["cluster_text3"]

        # ë‰´ìŠ¤ íŠ¸ë Œë“œ ìš”ì•½ ë°ì´í„° ìºì‹±
        if "trends" not in st.session_state:
            st.session_state["trends"] = {
                "trend1": summarize_cluster(cluster_text1, user_job, user_company, news_keywords[0]).split('\n\n'),
                "trend2": summarize_cluster(cluster_text2, user_job, user_company, news_keywords[1]).split('\n\n'),
                "trend3": summarize_cluster(cluster_text3, user_job, user_company, news_keywords[2]).split('\n\n')
            }

        trend1 = st.session_state["trends"]["trend1"]
        trend2 = st.session_state["trends"]["trend2"]
        trend3 = st.session_state["trends"]["trend3"]

        # trend_idx ì„¸ì…˜ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
        if "trend_idx" not in st.session_state:
            st.session_state["trend_idx"] = {
                1: list(map(int, re.findall(r'\d+', trend1[0]))),
                2: list(map(int, re.findall(r'\d+', trend2[0]))),
                3: list(map(int, re.findall(r'\d+', trend3[0])))
            }

        trend_idx1 = st.session_state["trend_idx"][1]
        trend_idx2 = st.session_state["trend_idx"][2]
        trend_idx3 = st.session_state["trend_idx"][3]

        def view_topic(keyword_idx, topic_idx):
            news_dict = {
                1: st.session_state["news_summaries"]["news_dict1"],
                2: st.session_state["news_summaries"]["news_dict2"],
                3: st.session_state["news_summaries"]["news_dict3"]
            }

            cluster_text = {
                1: cluster_text1,
                2: cluster_text2,
                3: cluster_text3
            }

            trend = {
                1: trend1,
                2: trend2,
                3: trend3
            }

            trend_idx = {
                1: trend_idx1,
                2: trend_idx2,
                3: trend_idx3
            }

            cluster_idx = trend_idx[keyword_idx][topic_idx-1]

            # ê²€ìƒ‰ì— ì“¸ full text
            full_text = cluster_text[keyword_idx].split("\n\n")[cluster_idx-1][10:]

            df = news_dict[keyword_idx][cluster_idx]
            titles = df['Title'].tolist()
            urls = df['Content URL'].tolist()

            topic = trend[keyword_idx][topic_idx].split('\n')[0][3:]

            # ì œëª©ê³¼ URLì„ í•œ ì¤„ì”© í‘œì‹œí•˜ëŠ” ë¬¸ìì—´ ìƒì„±
            articles = "\n".join([f"{titles[i]} {urls[i]}" for i in range(len(df))])

            return topic, full_text, articles

        # í† í”½ë³„ ì²´í¬ë°•ìŠ¤ ì„ íƒ ì²˜ë¦¬
        selected_topic = st.session_state.get("selected_topic", None)

        for i, keyword in enumerate(news_keywords):
            st.write(f"**í‚¤ì›Œë“œ: {keyword}**")
            trends = [trend1, trend2, trend3][i]

            for idx, topic in enumerate(trends[1:], 1):
                topic_lines = topic.split("\n")
                topic_title = topic_lines[0][3:].strip()  # ì œëª© ì¶”ì¶œ ë° ê³µë°± ì œê±°
                topic_summary = topic_lines[1].strip() if len(topic_lines) > 1 else "ìš”ì•½ ì—†ìŒ"

                col1, col2 = st.columns([5, 1])
                with col1:
                    st.write(f"{idx}. {topic_title}")
                    st.write(f"{topic_summary}")

                with col2:
                    checkbox = st.checkbox("", key=f"{keyword}_{idx}", value=(selected_topic == topic_title))
                    if checkbox:
                        st.session_state["selected_topic"] = topic_title

            st.write("---")

        # ì„ íƒí•œ í† í”½ì´ ìˆì„ ê²½ìš° ê¸°ì‚¬ ë° ì§ˆë¬¸ í‘œì‹œ
        if "selected_topic" in st.session_state:
            selected_topic = st.session_state["selected_topic"]

            st.success(f"ì„ íƒëœ í† í”½: {selected_topic.strip()}")

            # ëª¨ë“  íŠ¸ë Œë“œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê²€ìƒ‰
            all_trends = [trend1[1:], trend2[1:], trend3[1:]]
            t_idx = None
            k_idx = None

            for idx, trend_list in enumerate(all_trends, 1):
                try:
                    t_idx = next(i for i, t in enumerate(trend_list) if selected_topic.strip() in t.strip()) + 1
                    k_idx = idx
                    break
                except StopIteration:
                    continue
            
            if k_idx and t_idx:
                selected_topic, full_text, related_articles = view_topic(k_idx, t_idx)
                st.text_area("ê´€ë ¨ ê¸°ì‚¬", related_articles, height=200)

                user_q2 = generate_q2(user_job, user_company, k_idx, selected_topic, full_text)
                for idx, question in enumerate(user_q2, 1):
                    st.write(f"{question}")

    with tab3:
        st.subheader("ë‹¨ìˆœ ê¸°ìˆ  ì§ˆë¬¸")
        s_keywords = extract_keywords(st.session_state["job_posting"], max_keywords=10)
        user_q3 = generate_q3(user_job, s_keywords, st.session_state["job_posting"])
        st.write(user_q3)
